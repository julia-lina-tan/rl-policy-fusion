{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl-policy-fusion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqy+56GmEZGcrcA657kZ/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julia-lina-tan/rl-policy-fusion/blob/main/rl_policy_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFCKkXIgvEAl"
      },
      "source": [
        "# RL Policy Fusion\n",
        "\n",
        "This notebook trains PPO agents from Stable Baselines3 in the \"Cart Pole\" Gym environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUdP8r7-6zC"
      },
      "source": [
        "Set up git environment so we can commit local files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwt7ab76PN1M"
      },
      "source": [
        "import getpass\n",
        "\n",
        "# Configure git\n",
        "!git init\n",
        "!git config — global user.email 'julialtan5838@gmail.com'\n",
        "!git config — global user.name 'julia-lina-tan'\n",
        "!git add -A\n",
        "\n",
        "username='julia-lina-tan'\n",
        "password=getpass.getpass('password: ')\n",
        "!git remote add origin https://${username}:${password}@github.com/${username}/rl-policy-fusion.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3_sZfj6_BkF"
      },
      "source": [
        "Clear working directory of all existing folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ4sBMfN3SjY"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove all non system folders\n",
        "for dir in os.listdir():\n",
        "    if dir in ['.config', '.ipynb_checkpoints', '.git']:\n",
        "        continue\n",
        "    shutil.rmtree('../content/'+dir)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfxz0NUvvoOa"
      },
      "source": [
        "# Install Stable Baselines and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcTGFqDIunf_"
      },
      "source": [
        "pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJnQfBM7v4Tj"
      },
      "source": [
        "!apt install swig cmake\n",
        "!pip install stable-baselines3[extra] box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcVnCWi-X3Yf"
      },
      "source": [
        "Additional installations/imports for rendering Gym environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIxiHOPUYNej"
      },
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* \n",
        "!apt-get install imagemagick\n",
        "\n",
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1g4rGO_v-QZ"
      },
      "source": [
        "# Import RL policy and agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W20wEA_swBjk"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import PPO"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t9dbOoYwlLY"
      },
      "source": [
        "# Import Gym environment and instantiate agent\n",
        "\n",
        "Cart Pole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "\"*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over.* \n",
        "\n",
        "*A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMSWgrezZH2p"
      },
      "source": [
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Get cartpole class (NOTE: we need to import from source code to modify env variables)\n",
        "!git clone https://github.com/openai/gym.git\n",
        "\n",
        "MODULE_PATH = '../content/gym/gym/envs/classic_control/cartpole.py' \n",
        "MODULE_NAME = 'cartpole'\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)\n",
        "module = importlib.util.module_from_spec(spec)\n",
        "sys.modules[spec.name] = module \n",
        "spec.loader.exec_module(module)\n",
        "\n",
        "from cartpole import CartPoleEnv\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxcOUYxtqdCN"
      },
      "source": [
        "Make mass and pole length configurable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6aG7RGipzAn"
      },
      "source": [
        "def config_env(env, masscart=1.0, masspole=0.1, length=0.5):\n",
        "    env.masscart = masscart\n",
        "    env.masspole = masspole\n",
        "    env.total_mass = env.masscart + env.masspole\n",
        "    env.length = length\n",
        "    env.polemass_length = env.masspole * env.length"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak_JEPW2HZtT"
      },
      "source": [
        "We choose the MlpPolicy because input of Cart Pole is a feature vector, not images. \n",
        "\n",
        "This MLP has 2 layers of 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1L9Fi0BwktC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b89676e-71e5-42a3-e3ea-bd6c70c076fe"
      },
      "source": [
        "os.makedirs('../content/agent1', exist_ok=True)\n",
        "\n",
        "env = Monitor(CartPoleEnv())\n",
        "config_env(env, length=5.0)\n",
        "\n",
        "model = PPO('MlpPolicy', env, verbose=1, seed=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgBfBWsxqJE"
      },
      "source": [
        "We load a [helper function](https://stable-baselines.readthedocs.io/en/master/common/evaluation.html) to evaluate the agent, and define a plotting function to help visualise the rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLclohp6xsou"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "def plot_rewards(mean_reward, title=None):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards at episode')\n",
        "    plt.xticks(list(range(1, len(mean_reward)+1)))\n",
        "    plt.plot(list(range(1, len(mean_reward)+1)), mean_reward, marker='o')\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Q3aW8fxzHz"
      },
      "source": [
        "We evaluate the untrained random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fsUaMwMxypD"
      },
      "source": [
        "# Separate env for evaluation\n",
        "eval_env = Monitor(CartPoleEnv())\n",
        "\n",
        "# Random agent, before training\n",
        "ep_rewards, ep_steps = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LhSRJYlyB5K"
      },
      "source": [
        "## Train the agent and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kD29hiGyJyG"
      },
      "source": [
        "# Train the agent\n",
        "model.learn(total_timesteps=1e4)\n",
        "\n",
        "# Save the agent\n",
        "model.save('../content/agent1/model')\n",
        "\n",
        "# Can also delete and load the model afterwards\n",
        "# del model  \n",
        "# model = PPO.load('../content/agent1/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98XgAXXMyYxG"
      },
      "source": [
        "## Evaluate the trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrnJwAQBIrWj"
      },
      "source": [
        "On training environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fClv9jl5jTVp"
      },
      "source": [
        "ep_rewards, ep_steps = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x1y1WraIsfV"
      },
      "source": [
        "On evaluation environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAJ5fQF0Wgi3"
      },
      "source": [
        "ep_rewards, ep_steps = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGhlCCCGfFbY"
      },
      "source": [
        "## Render Environment\n",
        "\n",
        "Demonstrate policy by rendering in environment over a number of evaluation episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swzfTkvXfEl5"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import animation\n",
        "\n",
        "def save_frames_as_gif(frames, path='../content', filename='gym_animation.gif'):\n",
        "\n",
        "    #Mess with this to change frame size\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
        "\n",
        "def render_agent_in_env(agent, env, n_eval_episodes=5, path='../content', filename='gym_animation'):\n",
        "    for i in range(n_eval_episodes):\n",
        "      frames = []\n",
        "      obs = env.reset()\n",
        "      for t in range(500):\n",
        "\n",
        "          #Render to frames buffer\n",
        "          frame = np.array(env.render('rgb_array'))\n",
        "          cv2.putText(frame, text=f'Episode {i+1}', org=(50,50), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=0.8, color=(0,0,0))\n",
        "          frames.append(frame)\n",
        "          action, _states = model.predict(obs)\n",
        "          obs, rewards, done, info = env.step(action)\n",
        "          if done:\n",
        "            break\n",
        "      save_frames_as_gif(frames, path=path, filename=f'{filename}-ep{i+1}.gif')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCIq_qZ_7MK_"
      },
      "source": [
        "render_agent_in_env(model, env, n_eval_episodes=3, path='../content/agent1/', filename='trained-agent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYBx_PukEh-L"
      },
      "source": [
        "# Get Policy Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmtp_YbREjc_"
      },
      "source": [
        "# Get the parameters\n",
        "model_params = model.get_parameters()\n",
        "\n",
        "def get_policy_net(model_params, net='action'):\n",
        "    \"\"\"\n",
        "    Get either the action net or the value net representing the policy.\n",
        "\n",
        "    :param model_params: (dict) the model parameters\n",
        "    :param net: (str) the net type to return; either ``action`` or ``value``\n",
        "    \"\"\"\n",
        "    if net != 'action' and net != 'value':\n",
        "        raise ValueError('Must be either action net or value net')\n",
        "    return model_params.get('policy').get(net+'_net.weight')\n",
        "\n",
        "action_net = get_policy_net(model_params, net='action')\n",
        "\n",
        "# Create list to store policy nets of individual agents\n",
        "all_policies = []\n",
        "all_policies.append(action_net)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtVp1FSC3Jo7"
      },
      "source": [
        "# Introduce second agent\n",
        "\n",
        "We use a different random seed when creating another PPO model. We also alter the environment variables in some way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e210bE043JI-",
        "outputId": "b07a73a6-b7b5-4c89-93c5-f7e6c4273944"
      },
      "source": [
        "os.makedirs('../content/agent2', exist_ok=True)\n",
        "\n",
        "env = Monitor(CartPoleEnv())\n",
        "config_env(env, masscart=10.0, masspole=5.0)\n",
        "\n",
        "model = PPO('MlpPolicy', env, verbose=1, seed=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8uOvYE3-rZ"
      },
      "source": [
        "## Evaluate the second agent\n",
        "\n",
        "We evaluate the untrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1yxetct4ATn"
      },
      "source": [
        "# Random agent, before training\n",
        "ep_rewards, ep_steps = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gshGiWqq4NE5"
      },
      "source": [
        "We train the agent and re-evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCcgLG5s4VQw"
      },
      "source": [
        "# Train the agent\n",
        "model.learn(total_timesteps=int(1e4))\n",
        "\n",
        "# Save the agent\n",
        "model.save('../content/agent2/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-WbqUUI24X"
      },
      "source": [
        "On training environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ns0hsSI7PR"
      },
      "source": [
        "ep_rewards, ep_steps = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZwrDYU1I9qs"
      },
      "source": [
        "On evaluation environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9cKZ1yiI--v"
      },
      "source": [
        "ep_rewards, ep_steps = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE1sqHD7JBOU"
      },
      "source": [
        "We can also render the agent in the evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0V0P3XOmhuE"
      },
      "source": [
        "render_agent_in_env(model, eval_env, n_eval_episodes=3, path='../content/agent2/', filename='trained-agent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLtqgCu6Lxw7"
      },
      "source": [
        "## Get policy net of second agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5JnJBerL8cv"
      },
      "source": [
        "model_params = model.get_parameters()\n",
        "action_net = get_policy_net(model_params, net='action')\n",
        "all_policies.append(action_net)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCU-ezMh8TS6"
      },
      "source": [
        "# Align policy nets using OT\n",
        "\n",
        "Compute the OT transport maps between the policy nets, then align the matching neurons to each other using this map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AQmUbATTRkI"
      },
      "source": [
        "# Install optimal transport requirements\n",
        "!pip install pot\n",
        "import ot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nizB2C0n8ZSH"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def group_policy_layers(all_policies):\n",
        "\n",
        "    k_layers = list(all_policies[0].shape)[0]\n",
        "    m_policies = len(all_policies)\n",
        "    n_neurons = list(all_policies[0].shape)[1]\n",
        "\n",
        "    # Create kxn matrix for k layers and m policies \n",
        "    policy_layers = np.empty((k_layers, m_policies, n_neurons))\n",
        "\n",
        "    # Fill each layer with the policies\n",
        "    for i in range(k_layers):\n",
        "        \n",
        "        for j, policy in enumerate(all_policies):\n",
        "\n",
        "            # Convert current layer in tensor to a vector\n",
        "            layer = tf.slice(policy, [i,0], [1,-1])\n",
        "            vector = tf.reshape(layer,[-1]).numpy()\n",
        "\n",
        "            # Put vector in matrix\n",
        "            policy_layers[i, j, :] = vector\n",
        "    return policy_layers\n",
        "  \n",
        "\n",
        "policy_layers = group_policy_layers(all_policies)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw3lQ9A7Ckd1"
      },
      "source": [
        "Compute all layer-wise transport maps and align neurons in each policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D099KP9N8v0R"
      },
      "source": [
        "def align_neurons(policy_layers):\n",
        "\n",
        "    # Compute layer-wise transport maps\n",
        "    for i, layer in enumerate(policy_layers):\n",
        "        n = layer[0].shape[0]\n",
        "\n",
        "        # Align second policy (source) to first policy (target)\n",
        "        Xs = np.stack((layer[1], np.zeros(n)), axis=-1)\n",
        "        Xt = np.stack((layer[0], np.zeros(n)), axis=-1)\n",
        "\n",
        "        # Compute optimal transport map using EMD, since matching whole neurons\n",
        "        ot_emd = ot.da.EMDTransport()\n",
        "        ot_emd.fit(Xs=Xs, Xt=Xt)\n",
        "\n",
        "        # Get source policy aligned to neuron positions of target policy\n",
        "        transp_Xt_emd = ot_emd.inverse_transform(Xt=Xt)\n",
        "        aligned_Xs = transp_Xt_emd[:,0]\n",
        "        \n",
        "        # Replace original with aligned policy\n",
        "        policy_layers[i,1,:] = aligned_Xs\n",
        "\n",
        "    return policy_layers\n",
        "\n",
        "aligned_layers = align_neurons(policy_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aKyUNKyez0g"
      },
      "source": [
        "## Fuse policy nets \n",
        "Fuse policies using averaging within each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWDgAcgNe6sq"
      },
      "source": [
        "def fuse_policies_by_layers(aligned_layers):\n",
        "\n",
        "    k_layers = aligned_layers.shape[0]\n",
        "    n_neurons = aligned_layers.shape[2]\n",
        "    \n",
        "    # Create kxn matrix for k layers and n neurons\n",
        "    fused_policy = np.empty((k_layers, n_neurons))\n",
        "    \n",
        "    # Fuse each layer separately\n",
        "    for i, layer in enumerate(aligned_layers):\n",
        "        a = layer[0]\n",
        "        b = layer[1]\n",
        "\n",
        "        # Average neurons across each policy per position\n",
        "        fused = np.mean([a, b], axis=0)\n",
        "\n",
        "        # Add to new fused policy\n",
        "        fused_policy[i,:] = fused\n",
        "\n",
        "    return fused_policy\n",
        "\n",
        "\n",
        "fused_policy = fuse_policies_by_layers(aligned_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZL2G28ViPkR"
      },
      "source": [
        "## Create new agent with fused policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8j7TlfTTAs6"
      },
      "source": [
        "# Push changed files to git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glRvP6-mTE6X"
      },
      "source": [
        "commit_msg = input('Enter a commit message: ')\n",
        "!git commit -m ${commit_msg}\n",
        "!git push -u origin main"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}