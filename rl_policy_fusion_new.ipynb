{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl-policy-fusion-new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPz4U1h8ou+h9CrCEWxX0RN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julia-lina-tan/rl-policy-fusion/blob/main/rl_policy_fusion_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bk_6dXpKQs_"
      },
      "source": [
        "# Intro\n",
        "\n",
        "This notebook trains agents from Stable Baselines3 in the \"Reacher\" Gym environment.\n",
        "\n",
        "1. An agent will be trained to reach for rewards in the 1st quadrant of the workspace\n",
        "2. An agent will be trained to reach for rewards in the 2nd quadrant of the workspace\n",
        "3. A fused agent will be initialised from these two previous agents using OT techniques\n",
        "4. The fused agent will relearn to reach for rewards in both the 1st and 2nd quadrants of the workspace\n",
        "5. The effectiveness of initialising via fusion (as compared to relearning from the original models or creating a new model from scratch) will be evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWTy9WJPLEQd"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Install Stable Baselines and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFspeY0xKHZP"
      },
      "source": [
        "!pip install stable-baselines3[extra]\n",
        "!apt install swig cmake\n",
        "!pip install stable-baselines3[extra] box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHHYXgUFLMyg"
      },
      "source": [
        "# Additional installations/imports for rendering Gym environment\n",
        "\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* \n",
        "!apt-get install imagemagick\n",
        "\n",
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk1P3TWgLXWl"
      },
      "source": [
        "Import RL policy, RL agents and wrappers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ZzYdybLa-r"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch as pt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5B2SHp9LpgE"
      },
      "source": [
        "Install [pybullet-gym](https://github.com/benelot/pybullet-gym) and import Reacher environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh8DzoMeDljU"
      },
      "source": [
        "!git clone https://github.com/openai/gym.git\n",
        "!cd gym && pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjl3Ev6sDt1N"
      },
      "source": [
        "!git clone https://github.com/benelot/pybullet-gym.git\n",
        "!cd pybullet-gym && pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKww53BcD1hl"
      },
      "source": [
        "import gym  # open ai gym\n",
        "import pybulletgym  # register PyBullet enviroments with open ai gym\n",
        "\n",
        "env = gym.make('ReacherPyBulletEnv-v0')\n",
        "# env.render() # call this before env.reset, if you want a window showing the environment\n",
        "env.reset()  # should return a state vector if everything worked"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9E3GiYF1aVv"
      },
      "source": [
        "## Rendering agent in environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUdPyPR_1eB6"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import animation\n",
        "\n",
        "def save_frames_as_gif(frames, path='../content', filename='gym_animation.gif'):\n",
        "\n",
        "    #Mess with this to change frame size\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
        "\n",
        "def render_agent_in_env(agent, env, n_eval_episodes=5, path='../content', filename='gym_animation'):\n",
        "    for i in range(n_eval_episodes):\n",
        "      frames = []\n",
        "      obs = env.reset()\n",
        "      for t in range(500):\n",
        "\n",
        "          #Render to frames buffer\n",
        "          frame = np.array(env.render('rgb_array'))\n",
        "          cv2.putText(frame, text=f'Episode {i+1}', org=(50,50), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=0.8, color=(0,0,0))\n",
        "          frames.append(frame)\n",
        "          action, _states = model.predict(obs)\n",
        "          obs, rewards, done, info = env.step(action)\n",
        "          if done:\n",
        "            break\n",
        "      save_frames_as_gif(frames, path=path, filename=f'{filename}-ep{i+1}.gif')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXIDvRSQzw7t"
      },
      "source": [
        "# Train agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUQOYcQ-z418"
      },
      "source": [
        "env = Monitor(env)\n",
        "model = PPO('MlpPolicy', env, verbose=1, seed=1)\n",
        "\n",
        "# TODO: configure the model\n",
        "\n",
        "model.learn(total_timesteps=1e4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAhC5IuWSThT"
      },
      "source": [
        "## Extracting policy from model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cv48R-YSI6A"
      },
      "source": [
        "model_params = model.get_parameters()\n",
        "\n",
        "def get_policy(model_params, net='action'):\n",
        "    \"\"\"\n",
        "    Get either action or value net representing the actor-critic policy.\n",
        "\n",
        "    :param model_params: (dict) the model parameters\n",
        "    :param net: (str) the net type to return; either ``action`` or ``value``\n",
        "    \"\"\"\n",
        "    if net != 'action' and net != 'value':\n",
        "        raise ValueError('Must be either action net or value net')\n",
        "    return model_params.get('policy').get(net+'_net.weight')\n",
        "\n",
        "action_net = get_policy(model_params, net='action')\n",
        "print(action_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtoTLXX10AEK"
      },
      "source": [
        "# Test agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCEi8XZ0B3p"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "def plot_rewards(mean_reward, title=None):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards at episode')\n",
        "    plt.xticks(list(range(1, len(mean_reward)+1)))\n",
        "    plt.plot(list(range(1, len(mean_reward)+1)), mean_reward, marker='o')\n",
        "    plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJJMWq_K0DU6"
      },
      "source": [
        "ep_rewards, ep_steps = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, return_episode_rewards=True)\n",
        "\n",
        "print(f'mean reward={(sum(ep_rewards)/len(ep_rewards)):.2f} +/- {np.std(ep_rewards):.2f}')\n",
        "plot_rewards(ep_rewards, title='Rewards over evaluation episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IEiH9ItS33a"
      },
      "source": [
        "Visualise the performance of the agent over a number of evaluation episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ4PDZDn1lAs"
      },
      "source": [
        "os.makedirs('../content/agent1', exist_ok=True)\n",
        "render_agent_in_env(model, env, n_eval_episodes=5, path='../content/agent1', filename='test')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}